{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"NO6N6Z_pPOdH","executionInfo":{"status":"ok","timestamp":1690660869648,"user_tz":420,"elapsed":196,"user":{"displayName":"Leo Mckee-Reid","userId":"11135756992882511991"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","# sklearn is a python library used for data mining/ML and analysis. We will use it minimally.\n","from sklearn.datasets import load_diabetes\n","from sklearn.model_selection import train_test_split\n","\n","# used only for visualization:\n","import pandas as pd # a python library used for data analysis\n","import matplotlib as mpl # a python library used for creating visuals\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# 1. Load the data"],"metadata":{"id":"qYdojeTOs_4u"}},{"cell_type":"code","source":["# Load the dataset from sklearn\n","data = load_diabetes()\n","X, y = data.data, data.target"],"metadata":{"id":"0Q3hGeZHi9rM","executionInfo":{"status":"ok","timestamp":1690660871618,"user_tz":420,"elapsed":216,"user":{"displayName":"Leo Mckee-Reid","userId":"11135756992882511991"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# 1.1. Understanding the dataset\n","- How many individual data points are there?\n","- What are the variables?\n","- Learn more here: https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\n","    - When in doubt, remember RTFM: https://www.wikiwand.com/en/RTFM"],"metadata":{"id":"KFTM5zOELhu5"}},{"cell_type":"code","source":["print(f'Number of data points: {len(X)}')\n","print(f'Number of positive diabetes results: {np.sum(y > np.median(y))}')\n","#print(X)\n","#print(y)\n","\n","# Convert the dataset to a Pandas DataFrame to display a correlation matrix\n","feature_names = data.feature_names\n","print(f'Features: {feature_names}')\n","\n","# Put into pandas dataframe simply to create a correlation matrix\n","df = pd.DataFrame(data=np.c_[X, y], columns=feature_names + ['target'])\n","\n","# Calculate the correlation matrix\n","correlation_matrix = df.corr()\n","\n","# Plot the correlation matrix as a heatmap using Matplotlib\n","plt.figure(figsize=(8, 6))\n","\n","norm = mpl.colors.TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)\n","\n","plt.imshow(correlation_matrix, cmap='seismic', interpolation='nearest', norm=norm)\n","plt.colorbar()\n","\n","plt.title('Correlation Matrix')\n","plt.xticks(range(len(correlation_matrix)), correlation_matrix.columns, rotation=45)\n","plt.yticks(range(len(correlation_matrix)), correlation_matrix.columns)\n","plt.show()"],"metadata":{"id":"gyUgZEAQM7FG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Preprocess the data\n","Clean and modify data so that it's ready to be fed into the neural network for training.\n","\n","Here we will split the data into training and testing sets, then normalize the data.\n","- To \"Normalize\" the data is to change the input features such that they are on the same scale. In this case, we make all the means = 0 and std dev = 1\n","    - This \"stablizes\" the NN, allowing for easier convergence and therefore performance."],"metadata":{"id":"aS1sbCPKlA57"}},{"cell_type":"code","source":["# Convert target to binary labels (0 or 1)\n","y_binary = (y > np.median(y)).astype(np.float32)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Normalize the features\n","scaler = tf.keras.layers.experimental.preprocessing.Normalization()\n","X_train = scaler(X_train)\n","X_test = scaler(X_test)"],"metadata":{"id":"zIITxTiUfrMa","executionInfo":{"status":"ok","timestamp":1690660875790,"user_tz":420,"elapsed":435,"user":{"displayName":"Leo Mckee-Reid","userId":"11135756992882511991"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#2.1. Perform sanity checks\n","- How much data has been split?\n","- Make sure output feature is correctly 0 or 1\n","- Whatever else you're curious about!"],"metadata":{"id":"EKPRqjNHlVqy"}},{"cell_type":"code","source":["print(f'Actual split ratio: {len(X_test) / len(X_train)}')\n","print(len(X_train))"],"metadata":{"id":"IPmplPDtuUb0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Build and compile the model\n","Here we define what the neural network structure is - i.e., the architecture.\n","\n","Then we choose the optimizer and loss function that will be used during the training process.\n","- An **optimizer** is the algorithm that adjusts the parameters of a NN during training. Gradient descent is the basic optimization algo.\n","- A **loss function** is the way we measure how well the model's predictions match the known target/labels of the training set.\n","  - \"Binary cross entropy\" measures the difference between the predicted NN output at the known binary labels. There are other losses for non-binary output (we will see this below!)\n","- An **activation function** is ..."],"metadata":{"id":"w61EZbj4s5Zt"}},{"cell_type":"code","source":["# Create the TensorFlow model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"id":"MMG6FPkmP6H6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question:** What does this model summary actually show us?\n","\n","The NN has 3 layers:\n","- one input layer\n","- one hidden layer (with 16 neurons)\n","- one output layer (with 1 neuron).\n","\n","The \"params\" values indicate the number of trainable parameters in each layer (i.e. the weights and biases that the model learns during training).\n","\n","10(16) weights + 1(16) biases = 176"],"metadata":{"id":"u5Md2g_KqVEM"}},{"cell_type":"code","source":["# Let's actually look at what the neural network is (look at the weights and biases)\n","\n","# Print the weights and biases of each layer\n","for layer in model.layers:\n","    if hasattr(layer, 'weights'):\n","        weights, biases = layer.get_weights()\n","        print(\"Layer:\", layer.name)\n","        print(\"Weights:\")\n","        print(weights)\n","        print(\"Biases:\")\n","        print(biases)"],"metadata":{"id":"BAcBO08XBgRM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Train the model\n","Here we repeatedly send data through the NN, calculate how wrong the results are (the loss), then update the NN parameters via backpropagation to decrease the loss next iteration.\n","- **Iteration** is a single update of the models parameters using 1 batch of training data\n","- **Batch size** is the number of data samples that are processed together for 1 iteration.\n","- **Epoch** is one complete pass through the entire training dataset. The number of iterations per epoch is dependant on the batch size.\n","\n","\n","e.g. If a training set has 1000 samples, and we use a batch size of 10, then there will be 100 iterations per epoch.\n","- The higher the batch size, the fast the training generally (but you use more memory and will eventually decrease model performance). There is a sweet spot you have to find through experiments!"],"metadata":{"id":"jSmRrtQhH4KJ"}},{"cell_type":"code","source":["# Train the model\n","history = model.fit(X_train, y_train, epochs=300, batch_size=256, validation_split=0.1)"],"metadata":{"id":"wPu0FtoeP51H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that the model has been trained, let's look at the weights and biases again."],"metadata":{"id":"y9IIQ1XIgLTN"}},{"cell_type":"code","source":["# Print the weights and biases of each layer\n","for layer in model.layers:\n","    if hasattr(layer, 'weights'):\n","        weights, biases = layer.get_weights()\n","        print(\"Layer:\", layer.name)\n","        print(\"Weights:\")\n","        print(weights)\n","        print(\"Biases:\")\n","        print(biases)"],"metadata":{"id":"JUFYYcxHgP44"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Evaluate the trained NN\n","Here we check how well our model performs on the test set (the 20% we set aside and haven't touched until now).\n","\n","We will also plot the loss and accuracy for both the training and validation set throughout the training process.\n"],"metadata":{"id":"HIkzi1djIvoN"}},{"cell_type":"code","source":["# Evaluate the model on the test set\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n","\n","fig,ax=plt.subplots(1,2,figsize=(8,4))\n","ax[0].plot(history.history['loss'],label='train')\n","ax[0].plot(history.history['val_loss'],label='val')\n","ax[0].legend()\n","ax[0].set_ylabel('loss')\n","ax[1].plot(history.history['accuracy'],label='train')\n","ax[1].plot(history.history['val_accuracy'],label='val')\n","ax[1].legend()\n","ax[1].set_ylabel('acc')\n","plt.tight_layout()"],"metadata":{"id":"QBFJmCl0f58w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loss vs Accuracy:\n","\n","Loss measures how well the predictions match the known target/label (many ways of measuring -> e.g. binary cross entropy)\n","\n","Accuracy is the actual % of correct predictions."],"metadata":{"id":"8fR3k4jIHfSO"}},{"cell_type":"markdown","source":["# 6. Experimentation\n","We can now conduct a couple experiments/modifications to see if it's possible to improve our model accuracy."],"metadata":{"id":"_gN9lQ2bK2yj"}},{"cell_type":"markdown","source":["# 6.1. Experiment \\#1: Learning Rate\n","Learning rate, Î±, is a hyperparameter that determines the step size taken by the optimization algorithm while updating the model's parameters during training.\n","- Larger learning rate mean bigger updates to the parameters after each iteration (pro: faster training, con: risks overshooting and instability)\n","\n","\n","Note: the Adam optimizer is an adaptive learning rate optimization algorithm that dynamically adjusts the learning rate during training based on the historical gradients of the parameters."],"metadata":{"id":"RxwwaXe7Bcjn"}},{"cell_type":"code","source":["# Learning rates to experiment with\n","learning_rates = [0.0001, 0.001, 0.01, 0.1, 1]\n","\n","# Dictionary to store training and validation accuracies for each learning rate\n","accuracy_history = {}\n","\n","# Loop over each learning rate and train a model\n","for lr in learning_rates:\n","    print(f\"Training model with learning rate: {lr}\")\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n","        tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n","\n","    # Compile the model with the current learning rate\n","    optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    # Train the model\n","    history = model.fit(X_train, y_train, epochs=300, batch_size=256, validation_split=0.1, verbose=0)\n","\n","    # Store training and validation accuracies for this learning rate\n","    accuracy_history[lr] = {\n","        'train_accuracy': history.history['accuracy'],\n","        'val_accuracy': history.history['val_accuracy']\n","    }"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3NO3loaNBcBQ","executionInfo":{"status":"ok","timestamp":1690636733558,"user_tz":420,"elapsed":95829,"user":{"displayName":"Leo Mckee-Reid","userId":"11135756992882511991"}},"outputId":"9173a7c6-d4f4-45bb-e40b-c14f2804908e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training model with learning rate: 0.0001\n","Training model with learning rate: 0.001\n","Training model with learning rate: 0.01\n","Training model with learning rate: 0.1\n","Training model with learning rate: 1\n"]}]},{"cell_type":"code","source":["# Plot the accuracy curves for each learning rate\n","plt.figure(figsize=(10, 6))\n","for lr, accuracies in accuracy_history.items():\n","    plt.plot(accuracies['train_accuracy'], label=f'Train LR={lr}')\n","\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy for Different Learning Rates')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"fRzWEjEOCRyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Experiment \\#2: Model Architecture"],"metadata":{"id":"XPzQ5WYQRj1h"}},{"cell_type":"markdown","source":["Looking back at the correlation matrix, we can see that the \"sex\" input feature has the least impact on the target. Therefore, let's try dropping that from the input data."],"metadata":{"id":"lHcDMuFWmgRc"}},{"cell_type":"code","source":["print(X_train)"],"metadata":{"id":"vaoBZw2YoWxL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove 1st feature from the input data\n","X_train = np.delete(X_train, [1], axis=1) # axis=1 means we're deleting column 1 , not rows\n","X_test = np.delete(X_test, [1], axis=1)"],"metadata":{"id":"TQwuguf3me9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sanity check!\n","print(len(X_train[0]))"],"metadata":{"id":"mB1yN8usobZY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# New architecture with Flatten and Dropout layers\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n","    tf.keras.layers.Dense(16, activation='relu'),\n","    #tf.keras.layers.Dropout(0.3),                  # Add Dropout layer with 30% dropout rate\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"id":"RUwzJWeCRi5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","history = model.fit(X_train, y_train, epochs=300, batch_size=256, validation_split=0.1)"],"metadata":{"id":"d_Tysi1yqxR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the test set\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n","\n","# plot accur for train and val sets\n","fig,ax=plt.subplots(1,2,figsize=(8,4))\n","ax[0].plot(history.history['loss'],label='train')\n","ax[0].plot(history.history['val_loss'],label='val')\n","ax[0].legend()\n","ax[0].set_ylabel('loss')\n","ax[1].plot(history.history['accuracy'],label='train')\n","ax[1].plot(history.history['val_accuracy'],label='val')\n","ax[1].legend()\n","ax[1].set_ylabel('acc')\n","plt.tight_layout()"],"metadata":{"id":"-8MoB9-HfSZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2\n","Dataset: California Housing Dataset\n","- https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\n","\n","Input: age of house, avg income in area, location, etc.\n","\n","Output: Cost of House (a continuous value -> regression)"],"metadata":{"id":"ys2_RPF1gwU8"}},{"cell_type":"code","source":["import tensorflow as tf\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"],"metadata":{"id":"LWYR2_kWfWyz","executionInfo":{"status":"ok","timestamp":1690661444437,"user_tz":420,"elapsed":270,"user":{"displayName":"Leo Mckee-Reid","userId":"11135756992882511991"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Load the data\n","data = fetch_california_housing()\n","X, y = data.data, data.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create the Normalization layer\n","normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n","normalizer.adapt(X_train)\n","\n","# Normalize the features\n","X_train = normalizer(X_train)\n","X_test = normalizer(X_test)"],"metadata":{"id":"nhuyfbxa2oYG","executionInfo":{"status":"ok","timestamp":1690661448115,"user_tz":420,"elapsed":2269,"user":{"displayName":"Leo Mckee-Reid","userId":"11135756992882511991"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Build the model\n","model = tf.keras.models.Sequential([\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(1)\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='mean_squared_error')"],"metadata":{"id":"J-DxAyJy2oV8","executionInfo":{"status":"ok","timestamp":1690661451768,"user_tz":420,"elapsed":252,"user":{"displayName":"Leo Mckee-Reid","userId":"11135756992882511991"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Here we see the use of \"mean squared error\" as the loss function.\n","\n","**Mean squared error** measures the difference between predicted output and known output, penalizing larger error/difference more."],"metadata":{"id":"KjhdybdVR2WG"}},{"cell_type":"code","source":["# Train the model\n","model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)"],"metadata":{"id":"sLJR3zsq2oRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the test set\n","y_pred = model.predict(X_test).squeeze()\n","\n","# Calculate the mean squared error\n","mse = tf.reduce_mean(tf.square(y_test - y_pred))\n","print(\"Mean Squared Error:\", mse.numpy())\n","\n","# Plot results\n","plt.figure(figsize=(8, 6))\n","plt.scatter(y_test, y_pred, alpha=0.5)\n","plt.xlabel('Actual Prices')\n","plt.ylabel('Predicted Prices')\n","# let's also show the y=x line, which is where all the data would be if the model was perfect\n","plt.plot([0,5],[0,5],ls='--',c='k')\n","plt.show()"],"metadata":{"id":"DqCC2-RW6Z1v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that \"accuracy\" like we used before isn't actually a good way of analysing the model, since the output is continuous."],"metadata":{"id":"mGaBVj6AR0AD"}},{"cell_type":"code","source":[],"metadata":{"id":"JhOfvgIg2oOU"},"execution_count":null,"outputs":[]}]}